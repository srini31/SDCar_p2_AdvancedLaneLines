{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Starting the project\n",
    "\n",
    "Camera Calibration\n",
    "Advanced Lane Finding Project\n",
    "The goals / steps of this project are the following:\n",
    "\n",
    "- Compute the camera calibration matrix and distortion coefficients given a set of chessboard images.\n",
    "- Apply a distortion correction to raw images.\n",
    "- Use color transforms, gradients, etc., to create a thresholded binary image.\n",
    "- Apply a perspective transform to rectify binary image (\"birds-eye view\").\n",
    "- Detect lane pixels and fit to find the lane boundary.\n",
    "- Determine the curvature of the lane and vehicle position with respect to center.\n",
    "- Warp the detected lane boundaries back onto the original image.\n",
    "- Output visual display of the lane boundaries and numerical estimation of lane curvature and vehicle position."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### camera calibration using chessboard images\n",
    "### Calibrate camera, test on an image and show the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Code Part 1\n",
    "import numpy as np\n",
    "import cv2\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "\n",
    "%matplotlib qt\n",
    "%matplotlib inline\n",
    "\n",
    "# prepare object points, like (0,0,0), (1,0,0), (2,0,0) ....,(6,5,0)\n",
    "objp = np.zeros((6*9,3), np.float32)\n",
    "objp[:,:2] = np.mgrid[0:9, 0:6].T.reshape(-1,2)\n",
    "\n",
    "#map corners of 2D image (image points) to the 3D coordinates of the real undistorted chess board corners (object points)\n",
    "# Arrays to store object points and image points from all the images.\n",
    "objpoints = [] # 3d points in real world space\n",
    "imgpoints = [] # 2d points in image plane.\n",
    "\n",
    "# Make a list of calibration images\n",
    "images = glob.glob('camera_cal/calibration*.jpg')\n",
    "\n",
    "# Step through the list and search for chessboard corners\n",
    "for idx, fname in enumerate(images):\n",
    "    img = cv2.imread(fname)\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Find the chessboard corners\n",
    "    ret, corners = cv2.findChessboardCorners(gray, (9,6), None)\n",
    "\n",
    "    # If found, add object points, image points\n",
    "    if ret == True:\n",
    "        objpoints.append(objp)\n",
    "        imgpoints.append(corners)\n",
    "\n",
    "        # Draw and display the corners\n",
    "        cv2.drawChessboardCorners(img, (9,6), corners, ret)\n",
    "        #write_name = 'corners_found'+str(idx)+'.jpg'\n",
    "        #cv2.imwrite(write_name, img)\n",
    "        #cv2.imshow('img', img)\n",
    "        #cv2.waitKey(500)\n",
    "        #plt.imshow(img) # will show the image in the jupiter notebook\n",
    "\n",
    "#cv2.destroyAllWindows()\n",
    "#save the file\n",
    "### Calibrate camera, test on an image and show the results\n",
    "\n",
    "# Test undistortion on an image\n",
    "img = cv2.imread('camera_cal/calibration5.jpg')\n",
    "img_size = (img.shape[1], img.shape[0])\n",
    "\n",
    "# Do camera calibration given object points and image points\n",
    "ret, mtx, dist, rvecs, tvecs = cv2.calibrateCamera(objpoints, imgpoints, img_size,None,None)\n",
    "\n",
    "dst = cv2.undistort(img, mtx, dist, None, mtx)\n",
    "cv2.imwrite('camera_cal/test_image_undistorted.jpg',dst)\n",
    "\n",
    "# Save the camera calibration result for later use (we won't worry about rvecs / tvecs)\n",
    "dist_pickle = {}\n",
    "dist_pickle[\"mtx\"] = mtx\n",
    "dist_pickle[\"dist\"] = dist\n",
    "pickle.dump( dist_pickle, open( \"camera_cal/wide_dist_pickle.p\", \"wb\" ) )\n",
    "#dst = cv2.cvtColor(dst, cv2.COLOR_BGR2RGB)\n",
    "# Visualize undistortion\n",
    "f, (ax1, ax2) = plt.subplots(1, 2, figsize=(20,10))\n",
    "ax1.imshow(img)\n",
    "ax1.set_title('Original Image', fontsize=30)\n",
    "ax2.imshow(dst)\n",
    "ax2.set_title('Undistorted Image', fontsize=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calibrate camera, test on an image and show the results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Post utility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Code Part 2\n",
    "#some common plot methods\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class post_utility():\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.name = \"post_utility\"\n",
    "    \n",
    "    \n",
    "    def displayPlot(self, img, wid, hgt, wid_off, hgt_off, bottom_trim):\n",
    "        #print('dislpay plot utility')\n",
    "        # put the points on the image\n",
    "        plt.imshow(img, cmap = 'gray')\n",
    "        plt.plot(wid_off*0.95, hgt*bottom_trim, '.')   # bottom, left\n",
    "        plt.plot(3*wid_off*1.1, hgt*bottom_trim, '.') # bottom, right\n",
    "        plt.plot(0.55*wid, hgt_off, '.') # top, right\n",
    "        plt.plot(0.45*wid, hgt_off, '.') #top, left\n",
    "\n",
    "        #plt.imshow(warped, cmap = 'gray')\n",
    "        plt.plot(wid_off*1.1, hgt, '.')   # bottom, left\n",
    "        plt.plot(3*wid_off*0.9, hgt, '.') # bottom, right\n",
    "        plt.plot(3*wid_off*0.9, 10, '.') # top, right\n",
    "        plt.plot(wid_off*1.1, 10, '.') #top, left\n",
    "        \n",
    "        \n",
    "    # code to put the markings on the saved image --- to be debugged later\n",
    "    def displayPlotWarped(self, img, wid, hgt, wid_off, hgt_off, bottom_trim):\n",
    "        #print('dislpay plot warped utility')\n",
    "        # Create a named colour\n",
    "        red = [0,0,255]\n",
    "        blue = [255,0,0]\n",
    "        binary_img[int(wid_off*0.95), int(hgt*bottom_trim)]=red\n",
    "        binary_img[int(3*wid_off*1.1), int(hgt*bottom_trim)]=red\n",
    "        binary_img[int(0.55*wid), int(hgt_off)]=red\n",
    "        binary_img[int(0.45*wid), int(hgt_off)]=red\n",
    "\n",
    "        warped[int(wid_off*0.95), int(hgt*bottom_trim)]=blue\n",
    "        warped[int(3*wid_off*1.1), int(hgt*bottom_trim)]=blue\n",
    "        warped[int(0.55*wid), int(hgt_off)]=blue\n",
    "        warped[int(0.45*wid), int(hgt_off)]=blue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code for calculating gradient and color thresholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Code Part 3\n",
    "'''\n",
    "#create the pipeline of combined grayscale (gradient, sobel, RGB), color scale (HLS) using a binary operation to detect \n",
    "#lane lines better than canny edge detection. For example this is more effient when shadows are present and lane line \n",
    "color changes near a bridge\n",
    "\n",
    "for gray - use magnitude and direction gradient \n",
    "for color - use combination of R, S and H channels \n",
    "\n",
    "gray and R are good for right dotted line but not good if road color is white on the left side (on bridge)\n",
    "R, S and H are good for yellow lane (on the left or tright of road but not good for dotted line)\n",
    "'''\n",
    "#need these imports to run as standalone\n",
    "#import matplotlib.pyplot as plt\n",
    "#import matplotlib.image as mpimg\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "# Calculate gradient magnitude and apply threshold\n",
    "def mag_thresh(img, sobel_kernel=3, mag_thresh=(0, 255)):\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)  # Convert to grayscale\n",
    "    # Take the gradient in x and y separately\n",
    "    sobelx = cv2.Sobel(gray, cv2.CV_64F, 1, 0, ksize=sobel_kernel)\n",
    "    sobely = cv2.Sobel(gray, cv2.CV_64F, 0, 1, ksize=sobel_kernel)\n",
    "    gradmag = np.sqrt(sobelx ** 2 + sobely ** 2)  # Calculate the magnitude of the gradient\n",
    "    scale_factor = np.max(gradmag) / 255  \n",
    "    gradmag = (gradmag / scale_factor).astype(np.uint8)  # Scale to 8-bit (0 - 255) and convert to type = np.uint8  \n",
    "    mag_binary = np.zeros_like(gradmag)  # apply a threshold\n",
    "    mag_binary[(gradmag >= mag_thresh[0]) & (gradmag <= mag_thresh[1])] = 1   # Create binary mask where mag thresholds are met\n",
    "    return mag_binary  # Return this mask as your binary_output image\n",
    "\n",
    "\n",
    "# Calculate gradient direction and apply threshold\n",
    "def dir_thresh(img, sobel_kernel=3, thresh=(0, np.pi/2)):\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)  # Convert to grayscale\n",
    "    # Take the gradient in x and y separately\n",
    "    sobelx = cv2.Sobel(gray, cv2.CV_64F, 1, 0, ksize=sobel_kernel)\n",
    "    sobely = cv2.Sobel(gray, cv2.CV_64F, 0, 1, ksize=sobel_kernel)\n",
    "    # Take the absolute value of the x and y gradients\n",
    "    abs_sobelx = np.absolute(sobelx)\n",
    "    abs_sobely = np.absolute(sobely)\n",
    "    # Use np.arctan2(abs_sobely, abs_sobelx) to calculate the direction of the gradient\n",
    "    absgraddir = np.arctan2(abs_sobely, abs_sobelx)\n",
    "    # Create a binary mask where direction thresholds are met\n",
    "    dir_binary = np.zeros_like(absgraddir)\n",
    "    dir_binary[(absgraddir >= thresh[0]) & (absgraddir <= thresh[1])] = 1\n",
    "    return dir_binary  # Return this mask as your binary_output image\n",
    "\n",
    "\n",
    "# Define a function that thresholds the R-channel of RGB\n",
    "def rgb_r_thresh(img, thresh=(200, 255)):\n",
    "    red_ch = img[:, :, 0]\n",
    "    #plt.imshow(red_ch, cmap='gray')\n",
    "    binary_output = np.zeros_like(red_ch)\n",
    "    binary_output[(red_ch > thresh[0]) & (red_ch <= thresh[1])] = 1\n",
    "    return binary_output\n",
    "\n",
    "\n",
    "# Define a function that thresholds the R-channel of RGB\n",
    "def rgb_g_thresh(img, thresh=(200, 255)): #thresh=(200, 255)\n",
    "    green_ch = img[:, :, 1]\n",
    "    #plt.imshow(green_ch, cmap='gray')\n",
    "    binary_output = np.zeros_like(green_ch)\n",
    "    binary_output[(green_ch > thresh[0]) & (green_ch <= thresh[1])] = 1\n",
    "    return binary_output\n",
    "\n",
    "# Define a function that thresholds the S-channel of HLS\n",
    "# Use exclusive lower bound (>) and inclusive upper (<=)\n",
    "def hls_s_thresh(img, thresh=(90, 255)):\n",
    "    hls = cv2.cvtColor(img, cv2.COLOR_RGB2HLS)   # Convert to HLS color space\n",
    "    s_ch = hls[:, :, 2]  # Extract S channel\n",
    "    binary_output = np.zeros_like(s_ch)\n",
    "    binary_output[(s_ch > thresh[0]) & (s_ch <= thresh[1])] = 1  # Apply a threshold to the S channel\n",
    "    return binary_output  # Return a binary image of threshold result\n",
    "\n",
    "\n",
    "# Define a function that thresholds the L-channel of HLS\n",
    "# Use exclusive lower bound (>) and inclusive upper (<=)\n",
    "def hls_l_thresh(img, thresh=(90, 255)):\n",
    "    #print('img:', img)\n",
    "    hls = cv2.cvtColor(img, cv2.COLOR_RGB2HLS)  # Convert to HLS color space\n",
    "    l_ch = hls[:, :, 1]  # Extract S channel\n",
    "    #plt.imshow(l_ch, cmap='gray')\n",
    "    #print('l_ch:',l_ch)\n",
    "    binary_output = np.zeros_like(l_ch)\n",
    "    binary_output[(l_ch > thresh[0]) & (l_ch <= thresh[1])] = 1  # Apply a threshold to the S channel\n",
    "    return binary_output  # Return a binary image of threshold result\n",
    "\n",
    "\n",
    "# Define a function that thresholds the H-channel of HLS\n",
    "def hls_h_thresh(img, thresh=(15, 100)):\n",
    "    hls = cv2.cvtColor(img, cv2.COLOR_RGB2HLS)  # Convert to HLS color space\n",
    "    h_ch = hls[:, :, 0] # Extract S channel\n",
    "    binary_output = np.zeros_like(h_ch)\n",
    "    binary_output[(h_ch > thresh[0]) & (h_ch <= thresh[1])] = 1  # Apply a threshold to the S channel\n",
    "    return binary_output  # Return a binary image of threshold result\n",
    "\n",
    "def hsv_v_thresh(img, thresh=(180, 255)):\n",
    "        hsv = cv2.cvtColor(img, cv2.COLOR_RGB2HSV)  # Convert to HLS color space\n",
    "        v_ch = hsv[:, :, 2]  # Extract S channel\n",
    "        binary_output = np.zeros_like(v_ch)\n",
    "        binary_output[(v_ch > thresh[0]) & (v_ch <= thresh[1])] = 1  # Apply a threshold to the V channel\n",
    "        return binary_output  # Return a binary image of threshold result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sliding window using convolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Code Part 4\n",
    "#------------\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import glob\n",
    "import cv2\n",
    "\n",
    "class sliding_window_conv():\n",
    "    \n",
    "    def __init__(self, window_width_in, window_height_in, margin_in, ncenters_in, xm_per_pix_in, ym_per_pix_in):\n",
    "        self.name = \"sliding_win_conv\"\n",
    "        # window settings\n",
    "        self.window_width = window_width_in  # search window width  # 50 \n",
    "        self.window_height = window_height_in  # 80 # Break image into 9 vertical layers since image height is 720\n",
    "        self.margin = margin_in # 100 # How much to slide left and right for searching\n",
    "    \n",
    "        self.recent_centers = []\n",
    "        self.lane_start_points = []\n",
    "        #m_per_pixel_x  # meters per pixel in x-axis\n",
    "        #m_per_pixel_y  # meters per pixel in y-axis\n",
    "        self.smoothing_factor = ncenters_in  #10  so the the curve does not jump around\n",
    "        # Define conversions in x and y from pixels space to meters\n",
    "        self.xm_per_pix = xm_per_pix_in  #3.7/700 # meters per pixel in x dimension\n",
    "        self.ym_per_pix = ym_per_pix_in  #30/720 # meters per pixel in y dimension\n",
    "        \n",
    "\n",
    "    def window_mask(self, width, height, img_ref, center,level):\n",
    "        output = np.zeros_like(img_ref)\n",
    "        output[int(img_ref.shape[0]-(level+1)*height):int(img_ref.shape[0]-level*height),max(0,int(center-width/2)):min(int(center+width/2),img_ref.shape[1])] = 1\n",
    "        return output\n",
    "\n",
    "    \n",
    "    #if not white pixcel is found, windows are moving to left, should stay at previous location itself\n",
    "    # otherwise for next iteration, it will move away\n",
    "    # also can check if its equidistant from left lane -- in case the left lane pixels match the window, use it as reference\n",
    "    #input is a warped image that has been set to perspective mode\n",
    "    def find_window_centroids(self, image): #, window_width, window_height, margin\n",
    "\n",
    "        window_width = self.window_width\n",
    "        window_height = self.window_height\n",
    "        margin = self.margin\n",
    "        \n",
    "        window_centroids = [] # Store the (left,right) window centroid positions per level\n",
    "        window = np.ones(window_width) # Create our window template that we will use for convolutions\n",
    "\n",
    "        # First find the two starting positions for the left and right lane by using np.sum to get the vertical image slice\n",
    "        # and then np.convolve the vertical image slice with the window template \n",
    "\n",
    "        # Sum quarter bottom of image to get slice, could use a different ratio\n",
    "        l_sum = np.sum(image[int(3*image.shape[0]/4):,:int(image.shape[1]/2)], axis=0)\n",
    "        l_center = np.argmax(np.convolve(window,l_sum))-window_width/2\n",
    "        r_sum = np.sum(image[int(3*image.shape[0]/4):,int(image.shape[1]/2):], axis=0)\n",
    "        r_center = np.argmax(np.convolve(window,r_sum))-window_width/2+int(image.shape[1]/2)\n",
    "\n",
    "        # Add what we found for the first layer\n",
    "        window_centroids.append((l_center,r_center))\n",
    "        \n",
    "        #---------------------------------------------------\n",
    "        # minimize lane starting point drift between frames\n",
    "        # if this works, may be we can skip some frames but not sure how curvature at farther end is impacted\n",
    "        #find average start points for the lanes\n",
    "        self.lane_start_points.append((l_center,r_center))\n",
    "        avg_start_points = np.average(self.lane_start_points[-20:], axis = 0) # 10\n",
    "        \n",
    "        #if start points are 25 points away from average set to average\n",
    "        if(abs(l_center - avg_start_points[0]) >= 25):\n",
    "            l_center = avg_start_points[0]\n",
    "            \n",
    "        if(abs(r_center - avg_start_points[1]) >= 25):\n",
    "            r_center = avg_start_points[1]\n",
    "        \n",
    "        #print('avg start points 1: ',avg1)\n",
    "        #print('window_centroids 1: ', window_centroids)\n",
    "        #---------------------------------------------------\n",
    "        \n",
    "        # Go through each layer looking for max pixel locations\n",
    "        for level in range(1,(int)(image.shape[0]/window_height)):\n",
    "            # convolve the window into the vertical slice of the image\n",
    "            image_layer = np.sum(image[int(image.shape[0]-(level+1)*window_height):int(image.shape[0]-level*window_height),:], axis=0)\n",
    "            conv_signal = np.convolve(window, image_layer)\n",
    "            # Find the best left centroid by using past left center as a reference\n",
    "            # Use window_width/2 as offset because convolution signal reference is at right side of window, not center of window\n",
    "            offset = window_width/2\n",
    "            l_min_index = int(max(l_center+offset-margin,0))\n",
    "            l_max_index = int(min(l_center+offset+margin,image.shape[1]))\n",
    "            l_center = np.argmax(conv_signal[l_min_index:l_max_index])+l_min_index-offset\n",
    "            # Find the best right centroid by using past right center as a reference\n",
    "            r_min_index = int(max(r_center+offset-margin,0))\n",
    "            r_max_index = int(min(r_center+offset+margin,image.shape[1]))\n",
    "            r_center = np.argmax(conv_signal[r_min_index:r_max_index])+r_min_index-offset\n",
    "            # Add what we found for that layer\n",
    "            window_centroids.append((l_center,r_center))\n",
    "\n",
    "        self.recent_centers.append(window_centroids)\n",
    "        #print('window_centroids 2: ', window_centroids)\n",
    "        #print('np.average(self.recent_centers[-self.smoothing_factor:], axis=0): ', np.average(self.recent_centers[-self.smoothing_factor:], axis=0))\n",
    "        \n",
    "        return np.average(self.recent_centers[-self.smoothing_factor:], axis=0)\n",
    "        #return window_centroids\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# List of steps performed in advanced_lane_find \n",
    "### Apply a distortion correction to raw images.\n",
    "### Use color transforms, gradients, etc., to create a thresholded binary image.\n",
    "### Detect lane pixels and fit to find the lane boundary\n",
    "### Determine the curvature of the lane and vehicle position with respect to center (curvature uses polyfit through lane pixels and then find curvature)\n",
    "### Warp the detected lane boundaries back onto the original image.\n",
    "### Output visual display of the lane boundaries and numerical estimation of lane curvature and vehicle position."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Code Part 5\n",
    "#------------\n",
    "import numpy as np\n",
    "import cv2\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "\n",
    "#def advanced_lane_find(img):\n",
    "class advanced_lane_find():\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.name = \"sliding_win_conv\"\n",
    "        self.leftx_val = []   # to save left poly points\n",
    "        self.rightx_val = []\n",
    "        self.smoothing_factor_poly = 8 # 12 is also good\n",
    "        self.l_points_val = []   # to save left right lane points\n",
    "        self.r_points_val = []\n",
    "        \n",
    "    \n",
    "    def compute_lane(self, img):\n",
    "        # test the calibration and distortion correction on the test_images \n",
    "        # Make a list of calibration images\n",
    "        #images = glob.glob('test_images/test*.jpg')\n",
    "\n",
    "        #read the camera calibration data from the pickle file\n",
    "        camera_cal_pickle_file = \"camera_cal/wide_dist_pickle.p\"\n",
    "        infile = open(camera_cal_pickle_file,'rb')\n",
    "        new_dist_pickle = pickle.load(infile)\n",
    "        mtx = new_dist_pickle[\"mtx\"]\n",
    "        dist = new_dist_pickle[\"dist\"]\n",
    "        infile.close()\n",
    "        img_3d = img\n",
    "\n",
    "        #read the mtx, dist from pickle -- to be added\n",
    "        undistort = cv2.undistort(img, mtx, dist, None, mtx) #undistorted image\n",
    "        #test for distortion correction\n",
    "        img = undistort\n",
    "        ksize = 3 # Choose a larger odd number to smooth gradient measurements\n",
    "        mag_binary = mag_thresh(img, sobel_kernel=ksize, mag_thresh=(30, 100)) #mag_thresh=(0, 255)\n",
    "        ksize = 15\n",
    "        dir_binary = dir_thresh(img, sobel_kernel=ksize, thresh=(0.7, 1.2)) #15, thresh=(0.7, 1.3) 0, np.pi/2\n",
    "\n",
    "        #rgb_r_binary = rgb_r_thresh(img, thresh=(200, 255)) #200, 255\n",
    "        #rgb_g_binary = rgb_g_thresh(img, thresh=(200,255)) #200, 255\n",
    "\n",
    "        hls_h_binary = hls_h_thresh(img, thresh=(12, 80)) #15, 100\n",
    "        hls_l_binary = hls_l_thresh(img, thresh=(200, 255)) #15, 100\n",
    "        hls_s_binary = hls_s_thresh(img, thresh=(100, 255)) #90, 255)\n",
    "\n",
    "        hsv_v_binary = hsv_v_thresh(img, thresh=(180, 255))  # 90, 255)\n",
    "        # Combine the two or more binary thresholds\n",
    "        combined_binary = np.zeros_like(dir_binary)    \n",
    "        #combined_binary[( ((rgb_r_binary == 1) | (rgb_g_binary == 1)) |\n",
    "        #           (((hls_h_binary == 1) | (hls_s_binary == 1)) & (dir_binary == 1) )\n",
    "        #          )] = 1\n",
    "\n",
    "        combined_binary[ ( (mag_binary == 1) | (((hls_h_binary == 1)|(hls_s_binary == 1) ) ) ) & (hsv_v_binary == 1)] = 1\n",
    "\n",
    "        #plt.imshow(combined_binary, cmap='gray')\n",
    "        #print(\"./test_images/binary_\"+fname[12:])\n",
    "\n",
    "        binary_img = combined_binary.astype('uint8') * 255 \n",
    "        #cv2.imwrite('./test_images/binary_'+fname[12:], binary_img)  #combined_binary.astype('uint8') * 255\n",
    "\n",
    "        img = binary_img\n",
    "        #create the perspective transform of lanes lines\n",
    "        wid = img.shape[1]     # 1280\n",
    "        hgt = img.shape[0]     # 720\n",
    "        img_size = (wid, hgt)  # img_size = (gray.shape[1], gray.shape[0])  # Grab the image shape\n",
    "\n",
    "        wid_off = 0.25*wid   # width offset\n",
    "        hgt_off = 0.65*hgt #0.64*hgt     # height offset\n",
    "        bottom_trim = .95 #.96  .94 -- can be more as dashboard is curved\n",
    "        #0.6*wid = wid_off * 2.4;  0.4*wid= wid_off * 1.6\n",
    "        # source points \n",
    "        #4;3  -- order of points\n",
    "        #1;2\n",
    "        src = np.float32([ [wid_off*0.95, hgt*bottom_trim], [3*wid_off*1.1, hgt*bottom_trim], \n",
    "                           [0.55*wid, hgt_off ], [0.45*wid, hgt_off] ])\n",
    "        # destination points\n",
    "        #dst = np.float32([ [wid_off,hgt], [3*wid_off, hgt], [3*wid_off, 0], [wid_off, 0]])\n",
    "        dst = np.float32([ [wid_off*1.1,hgt], [3*wid_off*0.9, hgt], [3*wid_off*0.9, 0], [wid_off*1.1, 0]])\n",
    "\n",
    "        M = cv2.getPerspectiveTransform(src, dst)  # to create parallel lanes from original road\n",
    "        Minv = cv2.getPerspectiveTransform(dst, src) # to recreate perspective lanes\n",
    "\n",
    "        warped = cv2.warpPerspective(img, M, img_size, flags=cv2.INTER_LINEAR)\n",
    "\n",
    "        cv2.imwrite('./test_images/binary_'+fname[12:], binary_img)  #combined_binary.astype('uint8') * 255\n",
    "        cv2.imwrite('./test_images/pers_'+fname[12:], warped)\n",
    "        #print('filname: ', './test_images/pers_'+fname[12:])\n",
    "\n",
    "        #for documentation only\n",
    "        #tempPost = post_utility()\n",
    "        #tempPost.displayPlot(img, wid, hgt, wid_off, hgt_off, bottom_trim)\n",
    "        #------------------------------\n",
    "        # first create the sliding windows using convolution\n",
    "        # fit a polynomial through it and create the overlaid pictures\n",
    "        window_width = 25 \n",
    "        window_height = 80# 60 #80\n",
    "        margin = 25\n",
    "\n",
    "        # Read in a thresholded image\n",
    "        #warped = mpimg.imread('warped_example.jpg')\n",
    "        ncenters = 36#54 #18  #24#48 #15 \n",
    "        #  window_height=60, each frame 12 centers, average of 4 frames -> ncenters=48\n",
    "        xm_per_pix = 3.7/700 # meters per pixel in x dimension\n",
    "        ym_per_pix = 30/720 # meters per pixel in y dimension\n",
    "\n",
    "        window_conv = sliding_window_conv(window_width, window_height, margin, ncenters, xm_per_pix, ym_per_pix)\n",
    "        window_centroids = window_conv.find_window_centroids(warped)\n",
    "        myTemplate = np.zeros_like(warped) #to use for poly fill between lanes\n",
    "\n",
    "        ### Detect lane pixels and fit to find the lane boundary.\n",
    "        #this is for histogram to find location of lane pixels/lines in the image\n",
    "        # If we found any window centers\n",
    "        if len(window_centroids) > 0:\n",
    "            # Points used to draw all the left and right windows\n",
    "            l_points = np.zeros_like(warped)\n",
    "            r_points = np.zeros_like(warped)\n",
    "            leftx = []\n",
    "            rightx = []\n",
    "            # Go through each level and draw the windows \t\n",
    "            for level in range(0,len(window_centroids)):\n",
    "                # Window_mask is a function to draw window areas\n",
    "                l_mask = window_conv.window_mask(window_width,window_height,warped,window_centroids[level][0],level)\n",
    "                r_mask = window_conv.window_mask(window_width,window_height,warped,window_centroids[level][1],level)\n",
    "                # Add graphic points from window mask here to total pixels found \n",
    "                l_points[(l_points == 255) | ((l_mask == 1) ) ] = 255\n",
    "                r_points[(r_points == 255) | ((r_mask == 1) ) ] = 255\n",
    "                leftx.append(window_centroids[level][0]) #used for polyfit\n",
    "                rightx.append(window_centroids[level][1]) #used for polyfit\n",
    "\n",
    "            #----------------------------------------------------------\n",
    "            #add leftx, rightx vals to class global variable to average\n",
    "            self.leftx_val.append(leftx)\n",
    "            self.rightx_val.append(rightx)\n",
    "            # to save left right lane points\n",
    "            #self.l_points_val.append(l_points) \n",
    "            #self.r_points_val.append(r_points)\n",
    "            \n",
    "            #----------------Not needed for project, for understanding only-------------------\n",
    "            #----------------------------------------------------------\n",
    "            # Draw the results\n",
    "            template = np.array(r_points+l_points,np.uint8) # add both left and right window pixels together\n",
    "            #--------------------------------------------------\n",
    "            init_template = template # to save the polyfill\n",
    "            ##pointsToAvg = self.smoothing_factor_poly * npoints\n",
    "            ##get average of the points from smoothing_factor_poly(2) frames for the polynomial coefficients to smooth the lines\n",
    "            #pointsToAvg = 2 * npoints\n",
    "            #temp_l_points =  np.average(self.l_points_val[(-pointsToAvg):], axis=0)\n",
    "            #temp_r_points =  np.average(self.r_points_val[(-pointsToAvg):], axis=0)\n",
    "            #\n",
    "            #-----------------------------------------------------\n",
    "            zero_channel = np.zeros_like(template) # create a zero color channel\n",
    "            template = np.array(cv2.merge((zero_channel,template,zero_channel)),np.uint8) # make window pixels green\n",
    "            warpage= np.dstack((warped, warped, warped))*255 # making the original road pixels 3 color channels\n",
    "            #prints the windows that mark the lanes -- not used in video result\n",
    "            output = cv2.addWeighted(warpage, 1, template, 0.5, 0.0) # overlay the orignal road image with window results\n",
    "\n",
    "            # start code to fill the area between the lanes\n",
    "            #myTemplate = np.zeros_like(warped)\n",
    "            #print('template: ', template)\n",
    "            #print('template.shape: ', template.shape[0]) #720, 1280\n",
    "            rows = template.shape[0] #height\n",
    "            col = template.shape[1]  #width\n",
    "            check = 255\n",
    "            for row in range(rows): #720\n",
    "                try:\n",
    "                    templist = init_template[row].tolist()\n",
    "                    firstval = templist.index(check)\n",
    "                    secondval = len(templist) - 1 - templist[::-1].index(check)\n",
    "                    myTemplate[row][firstval:secondval] = 255\n",
    "                except ValueError:\n",
    "                    print('value is not in list..')\n",
    "            #.............\n",
    "            myTemplate = np.array(cv2.merge((zero_channel,myTemplate,zero_channel)),np.uint8)\n",
    "            #This output image prints the lanes marked with windows with polyfill in between, not used in video result\n",
    "            output = cv2.addWeighted(output, 1, myTemplate, 0.5, 0.0)\n",
    "            #cv2.imwrite('./test_images/output_conv_'+fname[12:], output)\n",
    "            #conv_warped = cv2.addWeighted(warpage, 1, myTemplate, 0.4, 0.0) # overlay the orignal road image with window results\n",
    "            #end code to fill area between the lanes\n",
    "            #----------------Not needed for project, for understanding only-------------------\n",
    "        else:\n",
    "            # If no window centers found, just display orginal road image\n",
    "            output = np.array(cv2.merge((warped,warped,warped)),np.uint8)\n",
    "\n",
    "        # write the convolution results\n",
    "        cv2.imwrite('./test_images/conv_'+fname[12:], output)\n",
    "\n",
    "        # assign warped to conv_warped, so that the area between the lanes is highlighted\n",
    "        #warped = conv_warped\n",
    "        #create the polyfit\n",
    "        # this has to be understood and updated\n",
    "        warp_hgt = warped.shape[0]\n",
    "        yvals = range(0, warp_hgt)\n",
    "        ploty = yvals\n",
    "        res_yvals = np.arange(warp_hgt - (window_height/2), 0, -window_height)  # can also use linspace which also returns an array\n",
    "\n",
    "        npoints = len(leftx)\n",
    "        #print('leftx: ', len(leftx), ' : ',leftx)\n",
    "        #print('rightx: ', len(rightx), ' : ',rightx)\n",
    "        #get average of the points from smoothing_factor_poly(2) frames for the polynomial coefficients to smooth the lines\n",
    "        pointsToAvg = self.smoothing_factor_poly * npoints\n",
    "        #pointsToAvg = 8 * npoints  #12\n",
    "        temp_leftx =  np.average(self.leftx_val[(-pointsToAvg):], axis=0)\n",
    "        temp_rightx =  np.average(self.rightx_val[(-pointsToAvg):], axis=0)\n",
    "        #print('tempLeftx: ', len(tempLeftx), ' : ',tempLeftx)\n",
    "        \n",
    "        #-----------------------------------------------------------------------------\n",
    "        #/////////////////////////////////////////////////////////////////\n",
    "        #recalculate the polyfill between lanes using the average leftx, rightx values\n",
    "        #to make sure the fill is between the new lines, otherwise it will show out of lanes\n",
    "        \n",
    "        window_centroids_avg = np.array((temp_leftx, temp_rightx)).T\n",
    "        myTemplateAvg = np.zeros_like(warped) #to use for poly fill between lanes\n",
    "        if len(window_centroids_avg) > 0:\n",
    "            # Points used to draw all the left and right windows\n",
    "            l_points = np.zeros_like(warped)\n",
    "            r_points = np.zeros_like(warped)\n",
    "            #leftx = [] #may not be needed here\n",
    "            #rightx = []  #may not be needed here\n",
    "            # Go through each level and draw the windows \t\n",
    "            for level in range(0,len(window_centroids_avg)):\n",
    "                # Window_mask is a function to draw window areas\n",
    "                l_mask = window_conv.window_mask(window_width,window_height,warped,window_centroids_avg[level][0],level)\n",
    "                r_mask = window_conv.window_mask(window_width,window_height,warped,window_centroids_avg[level][1],level)\n",
    "                # Add graphic points from window mask here to total pixels found \n",
    "                l_points[(l_points == 255) | ((l_mask == 1) ) ] = 255\n",
    "                r_points[(r_points == 255) | ((r_mask == 1) ) ] = 255\n",
    "                #leftx.append(window_centroids_avg[level][0]) #used for polyfit\n",
    "                #rightx.append(window_centroids_avg[level][1]) #used for polyfit\n",
    "\n",
    "            #----------------------------------------------------------\n",
    "            #----------------Not needed for project, for understanding only-------------------\n",
    "            # Draw the results\n",
    "            templateAvg = np.array(r_points+l_points,np.uint8) # add both left and right window pixels together\n",
    "            init_templateAvg = templateAvg\n",
    "            #-----------------------------------------------------\n",
    "            zero_channel = np.zeros_like(templateAvg) # create a zero color channel\n",
    "            templateAvg = np.array(cv2.merge((zero_channel,templateAvg,zero_channel)),np.uint8) # make window pixels green\n",
    "            warpageAvg = np.dstack((warped, warped, warped))*255 # making the original road pixels 3 color channels\n",
    "            #prints the windows that mark the lanes -- not used in video result\n",
    "            outputAvg = cv2.addWeighted(warpageAvg, 1, templateAvg, 0.5, 0.0) # overlay the orignal road image with window results\n",
    "\n",
    "            # start code to fill the area between the lanes\n",
    "            rows = templateAvg.shape[0] #height\n",
    "            col = templateAvg.shape[1]  #width\n",
    "            check = 255\n",
    "            for row in range(rows): #720\n",
    "                try: \n",
    "                    templist = init_templateAvg[row].tolist()\n",
    "                    firstval = templist.index(check)\n",
    "                    secondval = len(templist) - 1 - templist[::-1].index(check)\n",
    "                    myTemplateAvg[row][firstval:secondval] = 255\n",
    "                except ValueError:\n",
    "                    print('value is not in list..Avg')\n",
    "            #.............\n",
    "            myTemplateAvg = np.array(cv2.merge((zero_channel,myTemplateAvg,zero_channel)),np.uint8)\n",
    "            #This output image prints the lanes marked with windows with polyfill in between, not used in video result\n",
    "            outputAvg = cv2.addWeighted(outputAvg, 1, myTemplate, 0.5, 0.0)\n",
    "            #cv2.imwrite('./test_images/output_conv_'+fname[12:], output)\n",
    "            #conv_warpedAvg = cv2.addWeighted(warpage, 1, myTemplateAvg, 0.4, 0.0) # overlay the orignal road image with window results\n",
    "            #end code to fill area between the lanes\n",
    "            #----------------Not needed for project, for understanding only-------------------     \n",
    "        #end recalculate the polyfill\n",
    "        #///////////////////////////////////////////\n",
    "        #-----------------------------------------------------------------------\n",
    "        leftx = temp_leftx\n",
    "        rightx = temp_rightx\n",
    "        ###  Fit a second order polynomial to each using `np.polyfit` ###\n",
    "        left_fit = np.polyfit(res_yvals, leftx, 2)  # lefty, leftx\n",
    "        right_fit = np.polyfit(res_yvals, rightx, 2)  # righty, rightx\n",
    "\n",
    "        try:\n",
    "            left_fitx = left_fit[0] * np.power(ploty, 2) + left_fit[1] * ploty + left_fit[2]\n",
    "            right_fitx = right_fit[0] * np.power(ploty, 2) + right_fit[1] * ploty + right_fit[2]\n",
    "        except TypeError:\n",
    "            # Avoids an error if `left` and `right_fit` are still none or incorrect\n",
    "            print('The function failed to fit a line!')\n",
    "            left_fitx = 1 * np.power(ploty, 2) + 1 * ploty\n",
    "            right_fitx = 1 * np.power(ploty, 2) + 1 * ploty\n",
    "\n",
    "        left_fitx = np.array(left_fitx, np.int32) \n",
    "        right_fitx = np.array(right_fitx, np.int32) \n",
    "\n",
    "        # this has to updated later\n",
    "        left_lane = np.array(list(zip(np.concatenate((left_fitx-window_width/2, left_fitx[::-1]+window_width/2), axis=0), np.concatenate((yvals, yvals[::-1]), axis=0))), np.int32)\n",
    "        right_lane = np.array(list(zip(np.concatenate((right_fitx-window_width/2, right_fitx[::-1]+window_width/2), axis=0), np.concatenate((yvals, yvals[::-1]), axis=0))), np.int32)\n",
    "        middle_marker = np.array(list(zip(np.concatenate((right_fitx-window_width/2, right_fitx[::-1]+window_width/2), axis=0), np.concatenate((yvals, yvals[::-1]), axis=0))), np.int32)\n",
    "\n",
    "\n",
    "        ### Warp the detected lane boundaries back onto the original image.\n",
    "        ### Output visual display of the lane boundaries and numerical estimation of lane curvature and vehicle position.\n",
    "        out_img = np.dstack((warped, warped, warped)) * 255\n",
    "        #create red and blue lane lines\n",
    "        road = np.zeros_like(out_img)  # this is a blank image\n",
    "        cv2.fillPoly(road, np.int_([left_lane]), (255, 0, 0))\n",
    "        cv2.fillPoly(road, np.int_([right_lane]), (0, 0, 255))\n",
    "        #print('road: ', road)\n",
    "        #create a background white line so that the road lanes can be highlighted\n",
    "        road_bkg = np.zeros_like(out_img)  # actual image\n",
    "        cv2.fillPoly(road_bkg, [left_lane], (255, 255, 255))\n",
    "        cv2.fillPoly(road_bkg, [right_lane], (255, 255, 255))\n",
    "\n",
    "        road_warped = cv2.warpPerspective(road, Minv, img_size, flags=cv2.INTER_LINEAR)\n",
    "        road_warped_bkg = cv2.warpPerspective(road_bkg, Minv, img_size, flags=cv2.INTER_LINEAR)\n",
    "        #--------------\n",
    "        #to add green poly fill, use data from myTemplate\n",
    "        #lane_warped = cv2.warpPerspective(myTemplate, Minv, img_size, flags=cv2.INTER_LINEAR) #warp lane\n",
    "        lane_warped = cv2.warpPerspective(myTemplateAvg, Minv, img_size, flags=cv2.INTER_LINEAR) #warp lane\n",
    "        final_result = cv2.addWeighted(img_3d, 1, lane_warped, 0.4, 0.0) # overlay the orignal road image with window results\n",
    "        img_3d = final_result\n",
    "        #--------------\n",
    "        base = cv2.addWeighted(img_3d, 1.0, road_warped_bkg, -1.0,0.0)\n",
    "        result = cv2.addWeighted(base, 1.0, road_warped, 1.0,0.0)\n",
    "        #cv2.imwrite('./test_images/polyfill_'+fname[12:], result)\n",
    "\n",
    "        #----------------------------------------------------\n",
    "        # check how the region of interest marked by source points is performing\n",
    "        # create the polyfill of the initial source points box, No need to warp this\n",
    "        #init_box = np.zeros_like(out_img)  # actual image\n",
    "        #cv2.fillPoly(init_box, np.int_([src]), (255, 153, 51))  #$53, 255, 51\n",
    "        #result = cv2.addWeighted(result, 1.0, init_box, 0.6,0.0)\n",
    "        #-----------------------------------------------------\n",
    "    \n",
    "        #-------------------------------------------------------------------------------------\n",
    "        ### Determine the curvature of the lane and vehicle position with respect to center.\n",
    "        #curvature uses polyfit through lane pixels and then find curvature\n",
    "        # text on the image and video    \n",
    "        ploty = np.array(res_yvals, np.int32)\n",
    "        leftx_f = np.array(leftx, np.int32)\n",
    "        rightx_f = np.array(rightx, np.int32)\n",
    "        y_eval = np.max(ploty)\n",
    "\n",
    "        left_fit_cr = np.polyfit(ploty*ym_per_pix, leftx_f*xm_per_pix, 2)\n",
    "        right_fit_cr = np.polyfit(ploty*ym_per_pix, rightx_f*xm_per_pix, 2)\n",
    "\n",
    "        left_curverad = ((1 + (2*left_fit_cr[0]*y_eval*ym_per_pix + left_fit_cr[1])**2)**1.5) / np.absolute(2*left_fit_cr[0])\n",
    "        right_curverad = ((1 + (2*right_fit_cr[0]*y_eval*ym_per_pix + right_fit_cr[1])**2)**1.5) / np.absolute(2*right_fit_cr[0])\n",
    "\n",
    "        #calculate offset of the car on the road\n",
    "        camera_center = (left_fitx[-1] + right_fitx[-1])/2\n",
    "        center_diff = (camera_center - warped.shape[1]/2) * xm_per_pix\n",
    "        vehicle_pos = 'left'\n",
    "        if center_diff <= 0:\n",
    "            vehicle_pos = 'right'\n",
    "\n",
    "        cv2.putText(result, 'Radius of Curvature = '+ str(round(right_curverad, 3))+'(m)', (50,50), cv2.FONT_HERSHEY_SIMPLEX, 1, (255,255,255),2)\n",
    "        cv2.putText(result, 'Vehicle is '+str(abs(round(center_diff, 3)))+'(m) '+vehicle_pos+' of center', (50,100), cv2.FONT_HERSHEY_SIMPLEX, 1, (255,255,255),2)\n",
    "        cv2.imwrite('./test_images/final_'+fname[12:], result)\n",
    "\n",
    "        return result\n",
    "        #----------------------------------------------------------\n",
    "        # Colors in the left and right lane regions\n",
    "        #out_img[lefty, leftx] = [255, 0, 0]  # color left lane pixels -- this info is missing in convolution, get from window code\n",
    "        #out_img[righty, rightx] = [0, 0, 255] # color right lane pixels\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code Part 6\n",
    "#------------\n",
    "# Import everything needed to edit/save/watch video clips\n",
    "from moviepy.editor import VideoFileClip\n",
    "from IPython.display import HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Code Part 7\n",
    "#------------\n",
    "output_video = \"test_videos/submit_video.mp4\"\n",
    "input_video = \"test_videos/project_video.mp4\" \n",
    "#clip2 = VideoFileClip('test_videos/project_video.mp4').subclip(0,3)\n",
    "clip1 = VideoFileClip(\"test_videos/project_video.mp4\")#.subclip(25,42) #remove subclip after testing\n",
    "#0,10 0,35\n",
    "#15,42 is the window of error\n",
    "#25, 32 is the narrower window to check lane and car\n",
    "temp_lane = advanced_lane_find()\n",
    "output_clip = clip1.fl_image(temp_lane.compute_lane)\n",
    "#output_clip.write_videofile(output_video, audio=False)\n",
    "%time output_clip.write_videofile(output_video, audio=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code Part 8 - visualize project video\n",
    "#--------------------------------------\n",
    "HTML(\"\"\"\n",
    "<video width=\"960\" height=\"540\" controls>\n",
    "  <source src=\"{0}\">\n",
    "</video>\n",
    "\"\"\".format(output_video))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code Part 9\n",
    "#------------\n",
    "output_challenge_video = \"test_videos/submit_challenge_video.mp4\"\n",
    "input_challenge_video = \"challenge_video.mp4\" \n",
    "clip3 = VideoFileClip(input_challenge_video) #.subclip(0,4) #remove subclip after testing\n",
    "temp_lane = advanced_lane_find()\n",
    "output_ch_clip = clip3.fl_image(temp_lane.compute_lane)\n",
    "%time output_ch_clip.write_videofile(output_challenge_video, audio=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code Part 10\n",
    "#-------------\n",
    "HTML(\"\"\"\n",
    "<video width=\"960\" height=\"540\" controls>\n",
    "  <source src=\"{0}\">\n",
    "</video>\n",
    "\"\"\".format(output_challenge_video))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code Part 11\n",
    "#------------\n",
    "input_challenge_video = \"harder_challenge_video.mp4\" \n",
    "output_hard_challenge_video = \"test_videos/submit_hard_challenge_video.mp4\"\n",
    "clip3 = VideoFileClip(input_challenge_video) #.subclip(0,5) #remove subclip after testing\n",
    "temp_lane = advanced_lane_find()\n",
    "output_ch_clip = clip3.fl_image(temp_lane.compute_lane)\n",
    "%time output_ch_clip.write_videofile(output_hard_challenge_video, audio=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code Part 12\n",
    "#-------------\n",
    "HTML(\"\"\"\n",
    "<video width=\"960\" height=\"540\" controls>\n",
    "  <source src=\"{0}\">\n",
    "</video>\n",
    "\"\"\".format(output_hard_challenge_video))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BELOW IS FOR TESTING MODULES  \n",
    "#### This is not part of project code\n",
    "#### Apply a perspective transform to rectify binary image (\"birds-eye view\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# USED FOR TESTING ONLY\n",
    "#----------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# USED FOR TESTING ONLY\n",
    "#----------------------\n",
    "#mask region of interest -- needs debugging\n",
    " # draw region of interest\n",
    "    # Define a triangle region of interest \n",
    "    # The origin (x=0, y=0) is in the upper left in image processing\n",
    "    #left_bottom = [0, 539]\n",
    "    #right_bottom = [900, 300]\n",
    "    #apex = [400, 0]\n",
    "    # source points \n",
    "    #4;3  -- order of points\n",
    "    #1;2\n",
    "    # Fit lines (y=Ax+B) to identify the  3 sided region of interest\n",
    "    # np.polyfit() returns the coefficients [A, B] of the fit\n",
    "    \n",
    "    \n",
    "    fit_left = np.polyfit((src[0][0], src[3][0]), (src[0][1], src[3][1]), 1) # (left_bottom[0], apex[0]), (left_bottom[1], apex[1])\n",
    "    fit_right = np.polyfit((src[1][0], src[2][0]), (src[1][1], src[2][1]), 1) #(right_bottom[0], apex[0]), (right_bottom[1], apex[1])\n",
    "    fit_bottom = np.polyfit((src[0][0], src[1][0]), (src[0][1], src[1][1]), 1) #(left_bottom[0], right_bottom[0]), (left_bottom[1], right_bottom[1])\n",
    "    fit_top = np.polyfit((src[2][0], src[3][0]), (src[2][1], src[3][1]), 1)\n",
    "    \n",
    "    # Find the region inside the lines\n",
    "    XX, YY = np.meshgrid(np.arange(0, wid), np.arange(0, hgt))\n",
    "    region_thresholds = (YY > (XX*fit_left[0] + fit_left[1])) & \\\n",
    "                        (YY > (XX*fit_right[0] + fit_right[1])) & \\\n",
    "                        (YY < (XX*fit_bottom[0] + fit_bottom[1])) & \\\n",
    "                        (YY < (XX*fit_top[0] + fit_top[1]))\n",
    "                         \n",
    "    print('XX: ', XX)\n",
    "    print('YY: ', YY)\n",
    "    print('region_thresholds: ', region_thresholds)\n",
    "    \n",
    "    region_select = np.copy(img)\n",
    "    # Color pixels red which are inside the region of interest\n",
    "    region_select[region_thresholds] = [255, 0, 0]\n",
    "\n",
    "    # Display the image\n",
    "    plt.imshow(region_select)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Detect lane pixels and fit to find the lane boundary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "this is for histogram to find location of lane pixels/lines in the image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Determine the curvature of the lane and vehicle position with respect to center."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "curvature uses polyfit through lane pixels and then find curvature\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define a class to receive the characteristics of each line detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Define a class to receive the characteristics of each line detection\n",
    "# Was not able to implement this in the time provided\n",
    "class Line():\n",
    "    def __init__(self):\n",
    "        # was the line detected in the last iteration?\n",
    "        self.detected = False  \n",
    "        # x values of the last n fits of the line\n",
    "        self.recent_xfitted = [] \n",
    "        #average x values of the fitted line over the last n iterations\n",
    "        self.bestx = None     \n",
    "        #polynomial coefficients averaged over the last n iterations\n",
    "        self.best_fit = None  \n",
    "        #polynomial coefficients for the most recent fit\n",
    "        self.current_fit = [np.array([False])]  \n",
    "        #radius of curvature of the line in some units\n",
    "        self.radius_of_curvature = None \n",
    "        #distance in meters of vehicle center from the line\n",
    "        self.line_base_pos = None \n",
    "        #difference in fit coefficients between last and new fits\n",
    "        self.diffs = np.array([0,0,0], dtype='float') \n",
    "        #x values for detected line pixels\n",
    "        self.allx = None  \n",
    "        #y values for detected line pixels\n",
    "        self.ally = None  \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "### TEST ONLY # For testing the code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "    \n",
    "# to be updated 0\n",
    "# idea to create a single channel for r,g,b = [240,240,240], merge to get green\n",
    "def fillSingleChannel(img):\n",
    "    myTemplate = np.zeros_like(warped)\n",
    "    #print('template: ', template)\n",
    "    #print('template.shape: ', template.shape[0]) #720, 1280\n",
    "    #print('template[0]: ', template[0].max()) #720, 1280\n",
    "    #print('r_points: ', r_points) #720, 1280\n",
    "    rows = template.shape[0] #height\n",
    "    col = template.shape[1]  #width\n",
    "    check = 255\n",
    "    for row in range(rows): #720\n",
    "        templist = template[row].tolist()\n",
    "        firstval = templist.index(check)\n",
    "        secondval = len(templist) - 1 - templist[::-1].index(check)\n",
    "        myTemplate[row][firstval:secondval] = 255\n",
    "    #print(myTemplate)\n",
    "    #.............\n",
    "        \n",
    "# test the calibration and distortion correction on the test_images \n",
    "# Make a list of calibration images\n",
    "images = glob.glob('test_images/test*.jpg')\n",
    "\n",
    "#read the camera calibration data from the pickle file\n",
    "camera_cal_pickle_file = \"camera_cal/wide_dist_pickle.p\"\n",
    "infile = open(camera_cal_pickle_file,'rb')\n",
    "new_dist_pickle = pickle.load(infile)\n",
    "mtx = new_dist_pickle[\"mtx\"]\n",
    "dist = new_dist_pickle[\"dist\"]\n",
    "infile.close()\n",
    "\n",
    "# Step through the list and search for chessboard corners\n",
    "for idx, fname in enumerate(images):\n",
    "    img_3d = cv2.imread(fname)\n",
    "    \n",
    "    img = img_3d\n",
    "    #read the mtx, dist from pickle -- to be added\n",
    "    undistort = cv2.undistort(img, mtx, dist, None, mtx) #undistorted image\n",
    "    #test for distortion correction\n",
    "    #cv2.imwrite('./test_images/undist_'+fname[12:],dst)\n",
    "    #plt.imshow(dst)\n",
    "        \n",
    "    #print('dst: ',dst)\n",
    "    #Try different combinations and see what you get.\n",
    "    #For example, here is a selection for pixels where both the xx and yy gradients meet the threshold criteria,\n",
    "    #or the gradient magnitude and direction are both within their threshold values.  \n",
    "    img = undistort\n",
    "    ksize = 3 # Choose a larger odd number to smooth gradient measurements\n",
    "    mag_binary = mag_thresh(img, sobel_kernel=ksize, mag_thresh=(30, 100)) #mag_thresh=(0, 255)\n",
    "    ksize = 15\n",
    "    dir_binary = dir_thresh(img, sobel_kernel=ksize, thresh=(0.7, 1.2)) #15, thresh=(0.7, 1.3) 0, np.pi/2\n",
    "    \n",
    "    rgb_r_binary = rgb_r_thresh(img, thresh=(200, 255)) #200, 255\n",
    "    rgb_g_binary = rgb_g_thresh(img, thresh=(200,255)) #200, 255\n",
    "\n",
    "    hls_h_binary = hls_h_thresh(img, thresh=(12, 80)) #15, 100\n",
    "    hls_l_binary = hls_l_thresh(img, thresh=(200, 255)) #15, 100\n",
    "    hls_s_binary = hls_s_thresh(img, thresh=(100, 255)) #90, 255)\n",
    "\n",
    "    # Combine the two or more binary thresholds\n",
    "    combined_binary = np.zeros_like(dir_binary)\n",
    "    #combined[ ((mag_binary == 1) & (dir_binary == 1)) | ((rgb_r_binary == 1) & (hls_h_binary == 1) & (hls_s_binary == 1)) ] = 1\n",
    "  \n",
    "    #combined_binary[ (((mag_binary == 1) & ((rgb_r_binary == 1) | (rgb_g_binary == 1)) ) |\n",
    "    #      ((hls_h_binary == 1) & (hls_s_binary == 1)))  ] = 1\n",
    "\n",
    "    #& (dir_binary == 1) # may be add this for challenge video\n",
    "    #combined[((gradx == 1) & (grady == 1)) | ((mag_binary == 1) & (dir_binary == 1))] = 1\n",
    "    \n",
    "    combined_binary[( ((rgb_r_binary == 1) | (rgb_g_binary == 1)) |\n",
    "               (((hls_h_binary == 1) | (hls_s_binary == 1)) & (dir_binary == 1) )\n",
    "              )] = 1\n",
    "    \n",
    "    #combined_binary = mag_binary\n",
    "    plt.imshow(combined_binary, cmap='gray')\n",
    "    print(\"./test_images/binary_\"+fname[12:])\n",
    "        \n",
    "    binary_img = combined_binary.astype('uint8') * 255 \n",
    "    #cv2.imwrite('./test_images/binary_'+fname[12:], binary_img)  #combined_binary.astype('uint8') * 255\n",
    "    \n",
    "    img = binary_img\n",
    "    #create the perspective transform of lanes lines\n",
    "    wid = img.shape[1]     # 1280\n",
    "    hgt = img.shape[0]     # 720\n",
    "    img_size = (wid, hgt)  # img_size = (gray.shape[1], gray.shape[0])  # Grab the image shape\n",
    "\n",
    "    wid_off = 0.25*wid   # width offset\n",
    "    hgt_off = 0.64*hgt     # height offset\n",
    "    bottom_trim = .96 #.94 -- can be more as dashboard is curved\n",
    "    #0.6*wid = wid_off * 2.4;  0.4*wid= wid_off * 1.6\n",
    "    # source points \n",
    "    #4;3  -- order of points\n",
    "    #1;2\n",
    "    src = np.float32([ [wid_off*0.95, hgt*bottom_trim], [3*wid_off*1.1, hgt*bottom_trim], \n",
    "                       [0.55*wid, hgt_off ], [0.45*wid, hgt_off] ])\n",
    "    # destination points\n",
    "    #dst = np.float32([ [wid_off,hgt], [3*wid_off, hgt], [3*wid_off, 0], [wid_off, 0]])\n",
    "    dst = np.float32([ [wid_off*1.1,hgt], [3*wid_off*0.9, hgt], [3*wid_off*0.9, 0], [wid_off*1.1, 0]])\n",
    "    \n",
    "    print('src: ', src)\n",
    "    print('dst: ', dst)\n",
    "    M = cv2.getPerspectiveTransform(src, dst)  # to create parallel lanes from original road\n",
    "    Minv = cv2.getPerspectiveTransform(dst, src) # to recreate perspective lanes\n",
    "\n",
    "    warped = cv2.warpPerspective(img, M, img_size, flags=cv2.INTER_LINEAR)\n",
    "    \n",
    "    cv2.imwrite('./test_images/binary_'+fname[12:], binary_img)  #combined_binary.astype('uint8') * 255\n",
    "    cv2.imwrite('./test_images/pers_'+fname[12:], warped)\n",
    "    print('filname: ', './test_images/pers_'+fname[12:])\n",
    "    \n",
    "    #for documentation only\n",
    "    tempPost = post_utility()\n",
    "    tempPost.displayPlot(img, wid, hgt, wid_off, hgt_off, bottom_trim)\n",
    "    #cv2.imwrite('./test_images/pointsForPers_'+fname[12:], warped)\n",
    "    #------------------------------\n",
    "    # first create the sliding windows using convolution\n",
    "    # fit a polynomial through it\n",
    "    # create the overlaid pictures\n",
    "    \n",
    "    window_width = 25 \n",
    "    window_height = 60 #80\n",
    "    margin = 25\n",
    "    \n",
    "    # Read in a thresholded image\n",
    "    #warped = mpimg.imread('warped_example.jpg')\n",
    "    ncenters = 8 # 15\n",
    "    xm_per_pix = 3.7/700 # meters per pixel in x dimension\n",
    "    ym_per_pix = 30/720 # meters per pixel in y dimension\n",
    "        \n",
    "    window_conv = sliding_window_conv(window_width, window_height, margin, ncenters, xm_per_pix, ym_per_pix)\n",
    "    window_centroids = window_conv.find_window_centroids(warped)\n",
    "    myTemplate = np.zeros_like(warped) #to use for poly fill between lanes\n",
    "    \n",
    "    # If we found any window centers\n",
    "    if len(window_centroids) > 0:\n",
    "\n",
    "        # Points used to draw all the left and right windows\n",
    "        l_points = np.zeros_like(warped)\n",
    "        r_points = np.zeros_like(warped)\n",
    "        leftx = []\n",
    "        rightx = []\n",
    "        # Go through each level and draw the windows \t\n",
    "        for level in range(0,len(window_centroids)):\n",
    "            # Window_mask is a function to draw window areas\n",
    "            l_mask = window_conv.window_mask(window_width,window_height,warped,window_centroids[level][0],level)\n",
    "            r_mask = window_conv.window_mask(window_width,window_height,warped,window_centroids[level][1],level)\n",
    "            # Add graphic points from window mask here to total pixels found \n",
    "            l_points[(l_points == 255) | ((l_mask == 1) ) ] = 255\n",
    "            r_points[(r_points == 255) | ((r_mask == 1) ) ] = 255\n",
    "            leftx.append(window_centroids[level][0]) #used for polyfit\n",
    "            rightx.append(window_centroids[level][1]) #used for polyfit\n",
    "            \n",
    "        # Draw the results\n",
    "        template = np.array(r_points+l_points,np.uint8) # add both left and right window pixels together\n",
    "        init_template = template # to save the polyfill\n",
    "        zero_channel = np.zeros_like(template) # create a zero color channel\n",
    "        template = np.array(cv2.merge((zero_channel,template,zero_channel)),np.uint8) # make window pixels green\n",
    "        warpage= np.dstack((warped, warped, warped))*255 # making the original road pixels 3 color channels\n",
    "        output = cv2.addWeighted(warpage, 1, template, 0.5, 0.0) # overlay the orignal road image with window results\n",
    "\n",
    "        # start code to fill the area between the lanes\n",
    "        #myTemplate = np.zeros_like(warped)\n",
    "        #print('template: ', template)\n",
    "        #print('template.shape: ', template.shape[0]) #720, 1280\n",
    "        rows = init_template.shape[0] #height\n",
    "        col = init_template.shape[1]  #width\n",
    "        check = 255\n",
    "        for row in range(rows): #720\n",
    "            try:\n",
    "                templist = init_template[row].tolist()\n",
    "                firstval = templist.index(check)\n",
    "                secondval = len(templist) - 1 - templist[::-1].index(check)\n",
    "                myTemplate[row][firstval:secondval] = 255\n",
    "            except ValueError:\n",
    "                print('value is not in list..')\n",
    "        #.............\n",
    "        myTemplate = np.array(cv2.merge((zero_channel,myTemplate,zero_channel)),np.uint8)\n",
    "        output = cv2.addWeighted(output, 1, myTemplate, 0.5, 0.0) #save green poly fill inside sliding windows\n",
    "        cv2.imwrite('./test_images/output_conv_'+fname[12:], output)       \n",
    "        #conv_warped = cv2.addWeighted(warpage, 1, myTemplate, 0.4, 0.0) # overlay the orignal road image with window results\n",
    "        #end code to fill area between the lanes\n",
    "    # If no window centers found, just display orginal road image\n",
    "    else:\n",
    "        output = np.array(cv2.merge((warped,warped,warped)),np.uint8)\n",
    "\n",
    "    # Display the final results\n",
    "    #plt.imshow(output)\n",
    "    #plt.title('window fitting results')\n",
    "    #plt.show()\n",
    "    cv2.imwrite('./test_images/conv_'+fname[12:], output)\n",
    "        \n",
    "    # assign warped to conv_warped, so that the area between the lanes is highlighted\n",
    "    #warped # single dimension\n",
    "    #conv_warped three dimensions\n",
    "    #create the polyfit\n",
    "    # this has to be understood and updated\n",
    "    warp_hgt = warped.shape[0]\n",
    "    yvals = range(0, warp_hgt)\n",
    "    ploty = yvals\n",
    "    res_yvals = np.arange(warp_hgt - (window_height/2), 0, -window_height)  # can also use linspace which also returns an array\n",
    "        \n",
    "    ### TO-DO: Fit a second order polynomial to each using `np.polyfit` ###\n",
    "    left_fit = np.polyfit(res_yvals, leftx, 2)  # lefty, leftx\n",
    "    right_fit = np.polyfit(res_yvals, rightx, 2)  # righty, rightx\n",
    "\n",
    "    #left_fit = np.polyfit(lefty, leftx, 2)\n",
    "    #right_fit = np.polyfit(righty, rightx, 2)\n",
    "    \n",
    "    try:\n",
    "        left_fitx = left_fit[0] * np.power(ploty, 2) + left_fit[1] * ploty + left_fit[2]\n",
    "        right_fitx = right_fit[0] * np.power(ploty, 2) + right_fit[1] * ploty + right_fit[2]\n",
    "    except TypeError:\n",
    "        # Avoids an error if `left` and `right_fit` are still none or incorrect\n",
    "        print('The function failed to fit a line!')\n",
    "        left_fitx = 1 * np.power(ploty, 2) + 1 * ploty\n",
    "        right_fitx = 1 * np.power(ploty, 2) + 1 * ploty\n",
    "        \n",
    "    left_fitx = np.array(left_fitx, np.int32) \n",
    "    right_fitx = np.array(right_fitx, np.int32) \n",
    "    \n",
    "    left_lane = np.array(list(zip(np.concatenate((left_fitx-window_width/2, left_fitx[::-1]+window_width/2), axis=0), np.concatenate((yvals, yvals[::-1]), axis=0))), np.int32)\n",
    "    right_lane = np.array(list(zip(np.concatenate((right_fitx-window_width/2, right_fitx[::-1]+window_width/2), axis=0), np.concatenate((yvals, yvals[::-1]), axis=0))), np.int32)\n",
    "    middle_marker = np.array(list(zip(np.concatenate((right_fitx-window_width/2, right_fitx[::-1]+window_width/2), axis=0), np.concatenate((yvals, yvals[::-1]), axis=0))), np.int32)\n",
    "\n",
    "    \n",
    "    out_img = np.dstack((warped, warped, warped)) * 255  #image of lines  # same as warpage\n",
    "    #create red and blue lane lines\n",
    "    road = np.zeros_like(out_img)  # this is a 3d warped image\n",
    "    cv2.fillPoly(road, np.int_([left_lane]), (255, 0, 0))\n",
    "    cv2.fillPoly(road, np.int_([right_lane]), (0, 0, 255))\n",
    "    #print('rpoad: ', road)\n",
    "    #make the lane lines white to create background for red and blue lane lines\n",
    "    road_bkg = np.zeros_like(out_img)  # actual image\n",
    "    cv2.fillPoly(road_bkg, [left_lane], (255, 255, 255))\n",
    "    cv2.fillPoly(road_bkg, [right_lane], (255, 255, 255))\n",
    "\n",
    "    road_warped = cv2.warpPerspective(road, Minv, img_size, flags=cv2.INTER_LINEAR)\n",
    "    road_warped_bkg = cv2.warpPerspective(road_bkg, Minv, img_size, flags=cv2.INTER_LINEAR)\n",
    "\n",
    "    #--------------\n",
    "    #to add green poly fill\n",
    "    lane_warped = cv2.warpPerspective(myTemplate, Minv, img_size, flags=cv2.INTER_LINEAR) #warp lane\n",
    "    final_result = cv2.addWeighted(img_3d, 1, lane_warped, 0.4, 0.0) # overlay the orignal road image with window results\n",
    "    img_3d = final_result\n",
    "    #--------------\n",
    "    base = cv2.addWeighted(img_3d, 1.0, road_warped_bkg, -1.0,0.0)\n",
    "    result = cv2.addWeighted(base, 1.0, road_warped, 1.0,0.0)\n",
    "    \n",
    "     #cv2.fillPoly(result, np.int_([src]), (53, 255, 51))\n",
    "    init_box = cv2.warpPerspective(road, Minv, img_size, flags=cv2.INTER_LINEAR)\n",
    "    #cv2.imwrite('./test_images/finalZZZ_'+fname[12:], result)\n",
    "    cv2.imwrite('./test_images/polyfill_'+fname[12:], result)\n",
    "    \n",
    "    #-------------------------------------------------------------------------------------\n",
    "    # text on the image and video\n",
    "\n",
    "    ploty = res_yvals\n",
    "    ploty = np.array(res_yvals, np.int32)\n",
    "    leftx_f = np.array(leftx, np.int32)\n",
    "    rightx_f = np.array(rightx, np.int32)\n",
    "    y_eval = np.max(ploty)\n",
    "    \n",
    "    left_fit_cr = np.polyfit(ploty*ym_per_pix, leftx_f*xm_per_pix, 2)\n",
    "    right_fit_cr = np.polyfit(ploty*ym_per_pix, rightx_f*xm_per_pix, 2)\n",
    "\n",
    "    left_curverad = ((1 + (2*left_fit_cr[0]*y_eval*ym_per_pix + left_fit_cr[1])**2)**1.5) / np.absolute(2*left_fit_cr[0])\n",
    "    right_curverad = ((1 + (2*right_fit_cr[0]*y_eval*ym_per_pix + right_fit_cr[1])**2)**1.5) / np.absolute(2*right_fit_cr[0])\n",
    "\n",
    "    #calculate offset of the car on the road\n",
    "    camera_center = (left_fitx[-1] + right_fitx[-1])/2\n",
    "    center_diff = (camera_center - warped.shape[1]/2) * xm_per_pix\n",
    "    vehicle_pos = 'left'\n",
    "    if center_diff <= 0:\n",
    "        vehicle_pos = 'right'\n",
    "\n",
    "    cv2.putText(result, 'Radius of Curvature = '+ str(round(right_curverad, 3))+'(m)', (50,50), cv2.FONT_HERSHEY_SIMPLEX, 1, (255,255,255),2)\n",
    "    cv2.putText(result, 'Vehicle is '+str(abs(round(center_diff, 3)))+'(m) '+vehicle_pos+' of center', (50,100), cv2.FONT_HERSHEY_SIMPLEX, 1, (255,255,255),2)\n",
    "    cv2.imwrite('./test_images/final_'+fname[12:], result)\n",
    "\n",
    "    #an inefficient polyfill\n",
    "    #cv2.fillPoly(result, np.int_([src]), (53, 255, 51))\n",
    "    #cv2.imwrite('./test_images/finalZZZ_'+fname[12:], result)\n",
    "    #plt.imshow(result)\n",
    "    #plt.interactive(False)\n",
    "    #plt.show()\n",
    "    \n",
    "    #----------------------------------------------------------\n",
    "    # Colors in the left and right lane regions\n",
    "    #out_img[lefty, leftx] = [255, 0, 0]  # color left lane pixels -- this info is missing in convolution, get from window code\n",
    "    #out_img[righty, rightx] = [0, 0, 255] # color right lane pixels\n",
    "    \n",
    "    '''\n",
    "    # Stack each channel to view their individual contributions in green and blue respectively\n",
    "    # This returns a stack of the 5 binary images, whose components you can see as different colors\n",
    "    color_binary = np.dstack(( np.zeros_like(mag_binary), mag_binary, dir_binary, rgb_r_binary, hls_h_binary, hls_s_binary)) * 255\n",
    "\n",
    "    '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
